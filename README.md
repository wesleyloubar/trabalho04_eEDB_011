# trabalho04_eEDB_011

Trabalho 04 ministrado pelo professor Leandro Mendes Ferreira no segundo semestre de 2022 - Ingest√£o de Dados.

O trabalho consiste em realizar ETL:
1) ingerir dados de um CSV e uma API utilizando python
2) criar uma tabela fato utilizando esquema estrela 
3) Subir dados raw no S3
4) Realizar as transforma√ß√µes e inser√ß√µes na base dimensional
5) Utilizar AWS Glue para documentar Trusted e Refined

## üöÄ Come√ßando

Defininos as seguintes camadas: 

- `Raw`: Pasta para dados brutos
- `Raw_scripts`: Pasta de C√≥digos para dados brutos
- `Trusted`: Pasta para Dados tratados
- `Trusted_scripts`: Pasta de C√≥digos para Dados tratados
- `Refined`: Pasta para Dados Dados tratados e modelados
- `Refined_scripts`: Pasta de C√≥digos para Dados Dados tratados e modelados
- `source`: Dados para extra√ß√£o
- `drivers`: jar para execu√ß√£o de inser√ß√£o Spark 


## üìã Implementa√ß√£o
<img src="./images/1_arquitetura.png" width="75%">

* 1 - Nossa estrat√©gia foi recriar a estrutura de camadas exigidas para as atividades anteriores dentro do S3. Entendemos que o mais adequado seria criar um bucket para cada camada, mas como os c√≥digo j√° haviam sido modificados para um √∫nico bucket, mantivemos essa estrutura. 
<img src="./images/Imagem1.png" width="75%">

* 2 - AWS EMR para criar um cluster para executar c√≥digo Python e Spark.
 <img src="./images/4-clusters.png" width="75%">
 
* 3 - Base MySQL adquirido do provedor UOL em virtude de problemas para acessar a base Mysql do RDS da AWS.
 <img src="./images/2_bancodados.png" width="55%">

* 4 - Grafana para visualiza√ß√£o dos dados
 <img src="./images/6_grafana.jpg" width="75%">
 
* 5 - MySQL Workbench para verifica√ß√£o do processo de inser√ß√£o de dados na base uol.
 <br>
 * 6 - AWS Glue para documenta√ß√£o do cat√°logo de dados.
 Para gerar a documenta√ß√£o do cat√°logo de dados seguimos o seguinte processo:
 <br> Criar duas bases de dados, uma para cada reposit√≥rio do S3 que quer√≠amos documentar (trusted e refined)
 <br> Configurar dois crawlers para mapear a estruturas dos dados de cada reposit√≥rio.
 <br> Iniciar os crawlers que geraram as respectivas tabelas de metadados de cada reposit√≥rio
 <img src="./images/5_glue.png" width="75%">

<hr>

## Resultados
Acreditamos que atendemos todos os requisitos obrigat√≥rios propostos para a tarefa. 
<br> Tivemos problemas para acessar a base de dados, por isso recorremos √† base do provedor UOL. Al√©m disso, inserir o .jar de acesso do spark ao Mysql tamb√©m n√£o foi uma tarefa trivial. Mas conseguimos contornar essas dificuldades. Assim, conseguimos atingir a proposta de realizar todas as tarefas de processamento e carga dos dados em ambientes na cloud. 


## üõ†Ô∏è Constru√≠do com
* [Python](https://www.python.org/) - Linhas de c√≥digo utilizado para programa√ß√£o;
* [PySpark](https://spark.apache.org/docs/latest/api/python/) - Utilizado para ETL dos dados;
* [MySQL](https://www.mysql.com/) - Utilizado para ETL dos dados;
* [AWS S3](https://aws.amazon.com/pt/s3/) - Utilizado como reposit√≥rio de dados;
* [AWS EMR](https://aws.amazon.com/pt/emr/) - Utilizado para processamento dos dados;
* [AWS Glue](https://https://aws.amazon.com/pt/glue/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc/) - Utilizado para mapeamento do cat√°logo de dados;

## ‚úíÔ∏è Autores
* [Rodrigo Vitorino](https://github.com/digaumlv)
* [Thais Nabe](https://github.com/thaisnabe)
* [Vitor Marques](https://github.com/vitormrqs)
* [Wesley Louren√ßo Barbosa](https://github.com/wesleyloubar)
